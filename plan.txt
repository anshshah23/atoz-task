anshshah23: Customer Transaction Analytics – Assignment 
1. Problem Statement
A retail company maintains a transactional dataset containing over 1 million customer purchase records. Each record includes demographic information (age, gender, marital status), shopping behavior (segment, referral), geographic data (state), and transaction details (payment method, date, amount spent). Your task is to design a normalized relational database, define the necessary table names, create scenario-based questions, and list query titles that can be answered using the dataset.
2. Tables to Create 
- customers
- states
- segments
- employment_status
- payment_methods
- transactions
3. Simple Scenario-Based Questions
- Which customer segment generates the highest total revenue?
- What is the daily revenue and total number of transactions for the company?
- Do referral customers spend more compared to non-referral customers?
- What are the peak hours when most transactions occur?
- How does spending differ between male and female customers?
- Which states contribute the most revenue (Top 5)?
- How much revenue is generated by different age groups?
- Which payment method is used most frequently?
- Which employment status category contributes the highest spending?
- What is the overall transaction summary (count, revenue, average spend)?
 

4. Queries
Queries	Estimated Query Performance
Total Revenue by Customer Segment	25–40 ms
Transaction Count by Payment Method	15–30 ms
Average Spending Grouped by Gender	10–20 ms
Revenue Breakdown by Marital Status	20–35 ms
Top 5 States by Total Sales	30–45 ms
Referral vs Non-Referral Spending Comparison	5–10 ms
Revenue Contribution by Employment Status	25–40 ms
Age-Group Based Revenue Analysis	35–55 ms
Hourly Peak Sales Analysis	40–65 ms
Daily Sales Summary (Count, Revenue, Average Order Value)	35–60 ms


Server Environment (For Database Deployment & Performance Testing)
To simulate real-world production conditions, a dedicated cloud server was provisioned for hosting the analytical database and running performance benchmarks on the 1M-row dataset.

Server Credential:  
User: Ubuntu 
IP: 15.204.94.120
Password: BHGYGI@765
Dataset File: https://drive.google.com/file/d/1KdEzR8_5brJ4H1mW2wF9Oa9IXWjYzc4A/view?usp=sharing
Server Specifications
•	CPU: 12 Cores
•	RAM: 24 GB
•	Operating System: Ubuntu 22.04 LTS
•	Storage: SSD (High I/O throughput)


Have shared a server with you. You can host any open source DB on it like Mongo or PG or SB. Once you install, you can start working on it. 


lets make a plan in a .txt file first, we shall at first configure the server, then create a db instance, then do further work

GitHub Copilot: I'll help you create a comprehensive plan for this Customer Transaction Analytics project. Let me start by gathering context about your dataset and workspace. 

Read [](file:///c%3A/Users/ZBOOK/Desktop/Ansh%20NLP%20Project/atoz-task)

Research dataset structure

Searched for regex `^[a-zA-Z]` (`**/customer_spending_1M_2018_2025.csv`), 1 result

Searched for regex `^[0-9]` (`**/customer_spending_1M_2018_2025.csv`), 10 results

## Plan: Customer Transaction Analytics Database Implementation

Design and deploy a normalized PostgreSQL database on a remote Ubuntu server to analyze 1M+ customer transactions, create optimized analytical queries, and benchmark performance against specified targets (5-65ms query execution times).

### Steps

1. **Configure Remote Server** - SSH into Ubuntu server (15.204.94.120), update system packages, configure firewall rules (UFW), install PostgreSQL 15+, set up authentication, and configure `postgresql.conf` for 24GB RAM environment with optimized `shared_buffers`, `work_mem`, and parallel worker settings

2. **Download and Prepare Dataset** - Transfer customer_spending_1M_2018_2025.csv from Google Drive to server, validate data integrity (check for ~1M rows, 11 columns), analyze column data types and NULL patterns, and create data cleaning scripts for inconsistent `Employees_status` capitalization

3. **Create Normalized Database Schema** - Create database `customer_analytics`, define 6 tables (`customers`, `states`, `segments`, `employment_status`, `payment_methods`, `transactions`) with proper primary keys, foreign key constraints, and data types (use DECIMAL for `amount_spent`, TIMESTAMP for dates, handle NULLs appropriately)

4. **Build ETL Pipeline** - Create Python/psycopg2 scripts to: load lookup tables first (states, segments, employment_status, payment_methods), generate `customer_id` records from unique demographic combinations, bulk insert 1M transaction records using COPY command with FK mappings, and implement error handling for NULL values

5. **Create Strategic Indexes** - Build 15+ indexes including FK indexes on `transactions` table, B-tree index on `transaction_date`, composite indexes for common query patterns (segment+date, state+amount), partial indexes for referrals and high-value transactions, then run ANALYZE for query planner statistics

6. **Implement and Benchmark Queries** - Write SQL for 10 analytical queries (revenue by segment, peak hours analysis, gender spending comparison, top 5 states, age group revenue, referral vs non-referral, etc.), execute EXPLAIN ANALYZE to verify index usage, measure actual execution times, and document vs. target performance (25-65ms ranges)

### Further Considerations

1. **Customer Identity Strategy** - Treat each transaction as a unique customer (1M customers, 1 transaction each).

2. **Database Security** - Configure PostgreSQL to listen on private network only, create application-specific database user with limited privileges (no SUPERUSER), enable SSL connections, set strong password authentication, and consider SSH tunneling for remote access instead of exposing port 5432 publicly.

3. **Performance Monitoring** - Install `pg_stat_statements` extension to track query performance over time, set up automated VACUUM scheduling for transaction table maintenance, monitor disk I/O with `iotop`, and create dashboard queries to identify slow queries needing optimization.